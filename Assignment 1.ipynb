{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "sentence = 'I saw the man with a telescope.'\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(sentence) #used for printing purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting a path of dependency relations from the ROOT to a token\n",
    "\n",
    "The function takes as an input a sentence (string) and will output a List of list of tuple.\n",
    "Initially the doc object is created from the input sentence then with a for I cycle through all of the tokens inside the sentence. Now that I have a token I can generate its ancestors. Now I cycle through these ancestors and for each of them create the tuple of [dependency, token.text], which is immediately added to a list. Now that all I collected all of the path which goes from the token to its root, I reverse the list so that the root is always the first element of the list and finally I append the token text with its dependency itself to the bottom of the list so that there’s the full path coming from the root to a token. Now that the list for a token is complete, I add it to a list. This is repeated for all tokens in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token I with dependency -> [['ROOT', 'saw'], ['nsubj', 'I']]\n",
      "Token saw with dependency -> [['ROOT', 'saw']]\n",
      "Token the with dependency -> [['ROOT', 'saw'], ['dobj', 'man'], ['det', 'the']]\n",
      "Token man with dependency -> [['ROOT', 'saw'], ['dobj', 'man']]\n",
      "Token with with dependency -> [['ROOT', 'saw'], ['prep', 'with']]\n",
      "Token a with dependency -> [['ROOT', 'saw'], ['prep', 'with'], ['pobj', 'telescope'], ['det', 'a']]\n",
      "Token telescope with dependency -> [['ROOT', 'saw'], ['prep', 'with'], ['pobj', 'telescope']]\n",
      "Token . with dependency -> [['ROOT', 'saw'], ['punct', '.']]\n"
     ]
    }
   ],
   "source": [
    "def dependencyPath(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    dep=[]\n",
    "    for token in doc:\n",
    "        array=[]\n",
    "        ancestors = token.ancestors\n",
    "        for ancestor in ancestors:\n",
    "            anc=[ancestor.dep_, ancestor.text]\n",
    "            array.append(anc)\n",
    "        array.reverse()\n",
    "        array.append([token.dep_, token.text])\n",
    "        dep.append(array)\n",
    "    return dep\n",
    "\n",
    "dependency=dependencyPath(sentence)\n",
    "for i, dep in enumerate(dependency):\n",
    "    print (\"Token\" , doc[i], \"with dependency ->\", dep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract subtree of a dependents given a token\n",
    "\n",
    "The function takes as an input a sentence (string) and will output a list of list.\n",
    "Initially the doc object is created from the input sentence then with a for I cycle through all of the tokens inside the sentence. Now that I have a token I can generate its subtree. Now I cycle through the subtree and if the token is not the one I’m actually generating the subtree for, then I add them to a list. Finally, I add the subtree list to a final list that will be returned. This is repeated for all tokens in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token I with subtree -> []\n",
      "Token saw with subtree -> ['I', 'the', 'man', 'with', 'a', 'telescope', '.']\n",
      "Token the with subtree -> []\n",
      "Token man with subtree -> ['the']\n",
      "Token with with subtree -> ['a', 'telescope']\n",
      "Token a with subtree -> []\n",
      "Token telescope with subtree -> ['a']\n",
      "Token . with subtree -> []\n"
     ]
    }
   ],
   "source": [
    "def extractSubtree(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    sub=[]\n",
    "    for token in doc:\n",
    "        array=[]\n",
    "        #tree = doc[token.i].subtree\n",
    "        tree = token.subtree\n",
    "        for tr in tree:\n",
    "            if(token!=tr):\n",
    "                array.append(tr.text)\n",
    "        sub.append(array)\n",
    "    return sub\n",
    "\n",
    "subtree=extractSubtree(sentence)\n",
    "for i, tree in enumerate(subtree):\n",
    "    print (\"Token\" , doc[i], \"with subtree ->\", tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if a given list of tokens (segment of a sentence) forms a subtree\n",
    "\n",
    "The function takes as inputs an sequence (array) of string to be checked if they form a subtree and the sentence itself. After creating the doc object I create all the subtrees of the sentence with the previously defined function. Now I cycle through the doc using the enumerate function to be able to get the index and use it to check the right element in my subtree list, which is used to compare it to the given input sequence.\n",
    "To do the comparison I created a function that tries to remove an element of the second list while going through the other one and using each element of the latter to remove all elements of the other list. As soon as there is an “error” like if there is no element to remove (which means that one list has different elements from the other) it returns False. If for instance all of the elements of one list are also in the other, but the latter also has other elements, then it still returns False because the not of a non-empty list is still False. This function was created to still be able to recognize a list as subtree even if it wasn’t ordered correctly.\n",
    "Going back to the original function, as soon as a match is found the returned value is changed to True and the loop over all tokens is broken. Then we return the Boolean value to be displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sequence ['The', 'telescope', 'man'] is NOT a subtree of: \" I saw the man with a telescope. \"\n",
      "The sequence ['a', 'telescope'] is a subtree of: \" I saw the man with a telescope. \"\n"
     ]
    }
   ],
   "source": [
    "def compare(x, y):\n",
    "    try:\n",
    "        for elem in x:\n",
    "            y.remove(elem)\n",
    "    except ValueError:\n",
    "        return False\n",
    "    return not y\n",
    "\n",
    "def checkSubtree(tokens, sentence):\n",
    "    ret=False\n",
    "    doc = nlp(sentence)\n",
    "    subtree=extractSubtree(sentence)\n",
    "    for i, tk in enumerate(doc):\n",
    "        if compare(tokens, subtree[i]):\n",
    "            ret=True\n",
    "            break\n",
    "    return ret\n",
    "\n",
    "sequence1=['The', 'telescope', 'man']\n",
    "ret=checkSubtree(sequence1, sentence)\n",
    "if(ret):\n",
    "    print('The sequence',sequence1, 'is a subtree of: \"',sentence, '\"')\n",
    "else:\n",
    "    print('The sequence',sequence1, 'is NOT a subtree of: \"',sentence, '\"')\n",
    "    \n",
    "sequence2=['a', 'telescope']\n",
    "ret=checkSubtree(sequence2, sentence)\n",
    "if(ret):\n",
    "    print('The sequence',sequence2, 'is a subtree of: \"',sentence, '\"')\n",
    "else:\n",
    "    print('The sequence',sequence2, 'is NOT a subtree of: \"',sentence, '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify head of a span, given its tokens\n",
    "\n",
    "The function takes as input a portion of a sentence and returns the token that will correspond to the head of that span. Like before I cycle through all of the tokens to see if a token is actually the head of itself (since each token token points toward its head). Then I simply return the head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The head for span man with a  is: man\n"
     ]
    }
   ],
   "source": [
    "def headSpan(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    head='NONE'\n",
    "    for token in doc:\n",
    "        if token.head==token:\n",
    "            head=token\n",
    "    return head.text\n",
    " \n",
    "span = 'man with a'\n",
    "head = headSpan(span)\n",
    "print('The head for span',span,' is:',head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract sentence subject, direct object and indirect object spans\n",
    "\n",
    "The function takes as an input a sentence (string) and will output a dictionary with as three keys each for the sentence subject (or passive subject), direct object and indirect object (a.k.a. dative) and as element a list of lists where the inner lists contain the interested element and its span made by its the subtree.\n",
    "After loading the doc of the sentence I cycle again through the sentence tokens, and for each of them create a list of all its subtree (this time also including the token itself of course), but it will be added to the dictionary only if the token is a sentence subject, direct object or indirect object, if it happens to be in one of those cases then the subtree list gets added to another list that will then be added to the dictionary only after going through all of the tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsubj = ['I']\n",
      "iobj = []\n",
      "dobj = ['the man']\n"
     ]
    }
   ],
   "source": [
    "def subjectSpan(sentence):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc=nlp(sentence)\n",
    "    spans={}\n",
    "    array1=[]\n",
    "    array2=[]\n",
    "    array3=[]\n",
    "    for token in doc:\n",
    "        array = \"\"\n",
    "        space = \" \"\n",
    "        tree = token.subtree\n",
    "        for i, tr in enumerate(tree):\n",
    "            #only used to not put a space at the beginning \n",
    "            if i==0: \n",
    "                array = tr.text\n",
    "            else:\n",
    "                array=array+space +tr.text    \n",
    "        if (token.dep_ ==\"nsubj\" or token.dep_ ==\"nsubjpass\"):\n",
    "            array1.append(array)\n",
    "        elif (token.dep_==\"iobj\" or token.dep_==\"dative\"):\n",
    "            array2.append(array)\n",
    "        elif (token.dep_==\"dobj\"):\n",
    "            array3.append(array)\n",
    "    spans['nsubj']=array1\n",
    "    spans['iobj']=array2\n",
    "    spans['dobj']=array3\n",
    "    return spans\n",
    "\n",
    "spans=subjectSpan(sentence)\n",
    "for sub in spans:\n",
    "    print(sub, '=', spans.get(sub))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
